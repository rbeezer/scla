<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A Second Course in Linear Algebra       -->
<!--                                              -->
<!-- Copyright (C) 2004-2014  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<section xml:id="section-positive-semidefinite">
	<title>Positive Semi-Definite Matrices</title>

	<p>Positive semi-definite matrices (and their cousins, positive definite matrices) are square matrices which in many ways behave like non-negative (respectively, positive) real numbers.  These results will be useful as we study various matrix decompositions in Chapter<nbsp /><xref ref="chapter-decompositions" />.</p>

	<definition xml:id="definition-positive-semidefinite">  <!-- was PDM -->
		<title>Positive Semi-Definite Matrix</title>
		<statement>
			<p>A square matrix <m>A</m> of size <m>n</m> is <term>positive semi-definite</term> if <m>A</m> is Hermitian and for all <m>\vect{x}\in\complex{n}</m>, <m>\innerproduct{\vect{x}}{A\vect{x}}\geq 0</m>.</p>
		</statement>
	</definition>

	<p>For a definition of <term>positive definite</term> replace the inequality in the definition with a strict inequality, and exclude the zero vector from the vectors <m>\vect{x}</m> required to meet the condition.  Similar variations allow definitions of <term>negative definite</term> and <term>negative semi-definite</term>.</p>

	<!-- TODO: Characterization of positive semi-definite with positive determinants? -->
	<!-- TODO: Example:  Build positive semidefinite as a quadratic form, massage to get factorization into positive stuff, begin with A*A-adjoint and decompose -->

	<p>Our first theorem in this section gives us an easy way to build positive semi-definite matrices.</p>

	<theorem xml:id="theorem-matrix-adjoint-positive-definite"> <!-- was CPSM -->
		<title>Creating Positive Semi-Definite Matrices</title>
		<statement>
			<p>Suppose that <m>A</m> is any <m>m\times n</m> matrix.  Then the matrices <m>\adjoint{A}A</m> and <m>A\adjoint{A}</m> are positive semi-definite matrices.</p>
		</statement>

		<proof>
			<p>We will give the proof for the first matrix, the proof for the second is entirely similar.  First we check that <m>\adjoint{A}A</m> is Hermitian, with an appeal to <acroref type="definition" acro="HM" />, <me>\adjoint{\left(\adjoint{A}A\right)} =\adjoint{A}\adjoint{\left(\adjoint{A}\right)}=\adjoint{A}A</me></p>

			<p>Second, for any <m>\vect{x}\in\complex{n}</m>, <acroref type="theorem" acro="AIP" /> and <acroref type="theorem" acro="PIP" /> give, <me>\innerproduct{\vect{x}}{\adjoint{A}A\vect{x}} =\innerproduct{\adjoint{\left(\adjoint{A}\right)}\vect{x}}{A\vect{x}}=\innerproduct{A\vect{x}}{A\vect{x}}\geq 0</me> which is the second condition for a positive semi-definite matrix.</p>
		</proof>
	</theorem>

	<p>A statement very similar to the converse of this theorem is also true.  Any positive semi-definite matrix can be realized as the product of a square matrix, <m>B</m>, with its adjoint, <m>\adjoint{B}</m>.  (See Exercise<nbsp /><xref provisional="unwritten exercise about positive semi-definite and adjoints" /> after studying this entire section.)  The matrices <m>\adjoint{A}A</m> and <m>A\adjoint{A}</m> will be important later when we define singular values in Section<nbsp /><xref provisional="section-SVD" />.</p>

	<!-- TODO: Write an exercise: PSD is some matrix times its adjoint, similarity plus square roots? -->

	<p>Positive semi-definite matrices can also be characterized by their eigenvalues, without any mention of inner products.  This next result further reinforces the notion that positive semi-definite matrices behave like non-negative real numbers.</p>

	<theorem xml:id="theorem-positive-semidefinite-eigenvalues"> <!-- was EPSM -->
		<title>Eigenvalues of Positive Semi-definite Matrices</title>
		<statement>
			<p>Suppose that <m>A</m> is a Hermitian matrix.  Then <m>A</m> is a positive semi-definite matrix if and only if every eigenvalue <m>\lambda</m> of <m>A</m> has <m>\lambda\geq 0</m>.</p>
		</statement>

		<proof>
			<p>First notice first that since we are considering only Hermitian matrices in this theorem, it is always possible to compare eigenvalues with the real number zero, since eigenvalues of Hermitian matrices are all real numbers (<acroref type="theorem" acro="HMRE" />).</p>

			<p>(<imply />) Let <m>\vect{x}\neq 0</m> be an eigenvector of <m>A</m> for <m>\lambda</m>.  Since <m>A</m> is positive semi-definite, <me>\lambda\innerproduct{\vect{x}}{\vect{x}}=\innerproduct{\vect{x}}{\lambda\vect{x}} =\innerproduct{\vect{x}}{A\vect{x}}\geq 0</me> By <acroref type="theorem" acro="PIP" /> we know <m>\innerproduct{\vect{x}}{\vect{x}}\gt 0</m>, so we conclude that <m>\lambda\geq 0</m>.</p>

			<p>(<implyreverse />)  Let <m>n</m> denote the size of <m>A</m>.  Suppose that <m>\scalarlist{\lambda}{n}</m> are the eigenvalues of the Hermitian matrix <m>A</m>, each of which is non-negative.  Let <m>B=\set{\vectorlist{\vect{x}}{n}}</m> be a set of associated eigenvectors for these eigenvalues.  Since a Hermitian matrix is normal (<xref ref="definition-normal-matrix" />), <acroref type="theorem" acro="OBNM" /> allows us to choose <m>B</m> to also be an orthonormal basis of <m>\complex{n}</m>.  Choose any <m>\vect{x}\in\complex{n}</m> and let <m>\scalarlist{a}{n}</m> be the scalars guaranteed by the spanning property of the basis <m>B</m>, so <m>\vect{x}=\sum_{i=1}^{n}a_i\vect{x}_i</m>.  Since we have presumed <m>A</m> is Hermitian, we need only check the second condition of the definition.  The use of an orthonormal basis provides the simplification for the last equality.<md>
				<mrow>\innerproduct{\vect{x}}{A\vect{x}}
				&amp;=\innerproduct{\sum_{i=1}^{n}a_i\vect{x}_i}{A\sum_{j=1}^{n}a_j\vect{x}_j}</mrow>
				<mrow>&amp;=\innerproduct{\sum_{i=1}^{n}a_i\vect{x}_i}{\sum_{j=1}^{n}a_jA\vect{x}_j}</mrow>
				<mrow>&amp;=\innerproduct{\sum_{i=1}^{n}a_i\vect{x}_i}{\sum_{j=1}^{n}a_j\lambda_j\vect{x}_j}</mrow>
				<mrow>&amp;=\sum_{i=1}^{n}\sum_{j=1}^{n}\innerproduct{a_i\vect{x}_i}{a_j\lambda_j\vect{x}_j}</mrow>
				<mrow>&amp;=\sum_{i=1}^{n}\sum_{j=1}^{n}\conjugate{a_i}a_j\lambda_j\innerproduct{\vect{x}_i}{\vect{x}_j}</mrow>
				<mrow>&amp;=\sum_{i=1}^{n}\conjugate{a_i}a_i\lambda_i\innerproduct{\vect{x}_i}{\vect{x}_i}+\sum_{i=1}^{n}\sum_{\substack{j=1\\j\neq i}}^{n}\conjugate{a_i}a_j\lambda_j\innerproduct{\vect{x}_i}{\vect{x}_j}</mrow>
				<mrow>&amp;=\sum_{i=1}^{n}\conjugate{a_i}a_i\lambda_i</mrow> 
			</md> The expression <m>\conjugate{a_i}a_i</m> is the modulus of <m>a_i</m> squared, hence is always non-negative.  With the eigenvalues assumed non-negative, this final sum is clearly non-negative as well, as desired.</p>
		</proof>
	</theorem>

	<p>As positive semi-definite matrices are defined to be Hermitian, they are then normal and subject to orthonormal diagonalization (<acroref type="theorem" acro="OD" />).  Now consider the interpretation of orthonormal diagonalization as a rotation to principal axes, a stretch by a diagonal matrix and a rotation back (<acroref type="subsection" acro="OD.OD" />).  For a positive semi-definite matrix, the diagonal matrix has diagonal entries that are the non-negative eigenvalues of the original positive semi-definite matrix.  So the <q>stretching</q> along each axis is never a reflection.</p>

</section>
