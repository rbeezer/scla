<?xml version="1.0" encoding="UTF-8"?>

<!-- This file is part of the book                -->
<!--                                              -->
<!--      A Second Course in Linear Algebra       -->
<!--                                              -->
<!-- Copyright (C) 2004-2014  Robert A. Beezer    -->
<!-- See the file COPYING for copying conditions. -->

<section xml:id="section-SVD">
    <title>Singular Value Decomposition</title>

    <introduction>
        <p>The singular value decomposition is one of the more useful ways to represent any matrix, even rectangular ones.  We can also view the singular values of a (rectangular) matrix as analogues of the eigenvalues of a square matrix.</p>
    </introduction>

    <subsection acro= "MAP">
        <title>Matrix-Adjoint Products</title>

        <p>Our definitions and theorems in this section rely heavily on the properties of the matrix-adjoint products (<m>\adjoint{A}A</m> and <m>A\adjoint{A}</m>).  We start by examining some of the basic properties of these two positive semi-definite matrices.  Now would be a good time to review the basic facts about positive semi-definite matrices in Section<nbsp /><xref ref="section-positive-semidefinite" />.</p>

        <theorem xml:id="eigenvalues-matrix-adjoint-product"> <!-- was EEMAP -->
            <title>Eigenvalues and Eigenvectors of Matrix-Adjoint Product</title>
            <statement>
                <p>Suppose that <m>A</m> is an <m>m\times n</m> matrix and <m>\adjoint{A}A</m> has rank <m>r</m>.  Let <m>\scalarlist{\lambda}{p}</m> be the nonzero distinct eigenvalues of <m>\adjoint{A}A</m> and let <m>\scalarlist{\rho}{q}</m> be the nonzero distinct eigenvalues of <m>A\adjoint{A}</m>.  Then,
                <ol>
                    <li><m>p=q</m>.</li> 
                    <li> The distinct nonzero eigenvalues can be ordered such that <m>\lambda_i=\rho_i</m>, <m>1\leq i\leq p</m>.</li>
                    <li> Properly ordered, the algebraic multiplicities of the nonzero eigenvalues are identical, <m>\algmult{\adjoint{A}A}{\lambda_i}=\algmult{A\adjoint{A}}{\rho_i}</m>, <m>1\leq i\leq p</m>.</li>
                    <li> The rank of <m>\adjoint{A}A</m> is equal to the rank of <m>A\adjoint{A}</m>. </li>
                    <!-- TODO: List inside of list, once working -->
                    <li> There is an orthonormal basis, <m>\set{\vectorlist{x}{n}}</m> of <m>\complex{n}</m> composed of eigenvectors of <m>\adjoint{A}A</m> and an orthonormal basis, <m>\set{\vectorlist{y}{m}}</m> of <m>\complex{m}</m> composed of eigenvectors of <m>A\adjoint{A}</m> with the following properties.  Order the eigenvectors so that <m>\vect{x}_i</m>, <m>r+1\leq i\leq n</m> are the eigenvectors of <m>\adjoint{A}A</m> for the zero eigenvalue.  Let <m>\delta_i</m>, <m>1\leq i\leq r</m> denote the nonzero eigenvalues of <m>\adjoint{A}A</m>.  Then <m>A\vect{x}_i=\sqrt{\delta_i}\vect{y_i}</m>, <m>1\leq i\leq r</m> and <m>A\vect{x}_i=\zerovector</m>, <m>r+1\leq i\leq n</m>.  Finally, <m>\vect{y}_i</m>, <m>r+1\leq i\leq m</m>, are eigenvectors of <m>A\adjoint{A}</m> for the zero eigenvalue.
                    </li>
                </ol></p>
            </statement>

            <proof>
                <p>Suppose that <m>\vect{x}\in\complex{n}</m> is any eigenvector of <m>\adjoint{A}A</m> for a nonzero eigenvalue <m>\lambda</m>.  We will show that <m>A\vect{x}</m> is an eigenvector of <m>A\adjoint{A}</m> for the same eigenvalue, <m>\lambda</m>.  First, we ascertain that <m>A\vect{x}</m> is not the zero vector.
                <me>\innerproduct{A\vect{x}}{A\vect{x}}=\innerproduct{\vect{x}}{\adjoint{A}A\vect{x}}=\innerproduct{\vect{x}}{\lambda\vect{x}}=\lambda\innerproduct{\vect{x}}{\vect{x}}</me></p>

                <p>Since <m>\vect{x}</m> is an eigenvector, <m>\vect{x}\neq\zerovector</m>, and by <acroref type="theorem" acro="PIP" />, <m>\innerproduct{\vect{x}}{\vect{x}}\neq 0</m>.  As <m>\lambda</m> was assumed to be nonzero, we see that <m>\innerproduct{A\vect{x}}{A\vect{x}}\neq 0</m>.  Again, <acroref type="theorem" acro="PIP" /> tells us that <m>A\vect{x}\neq\zerovector</m>.</p>

                <p> Much of the sequel turns on the following simple computation.  If you ever wonder what all the fuss is about adjoints, Hermitian matrices, square roots, and singular values, return to this brief computation, as it holds the key.  There is much more to do in this proof, but after this it is mostly bookkeeping.  Here we go.  We check that <m>A\vect{x}</m> functions as an eigenvector of <m>A\adjoint{A}</m> for the eigenvalue <m>\lambda</m>,
                <me>\left(A\adjoint{A}\right)\left(A\vect{x}\right)
                   =A\left(\adjoint{A}A\right)\vect{x}
                   =A\lambda\vect{x}
                   =\lambda\left(A\vect{x}\right)</me></p>

                <p>That's it.  If <m>\vect{x}</m> is an eigenvector of <m>\adjoint{A}A</m>, for a <em>nonzero</em> eigenvalue, then <m>A\vect{x}</m> is an eigenvector for <m>A\adjoint{A}</m> for the same nonzero eigenvalue.  Let's see what this buys us.</p>

                <p><m>\adjoint{A}A</m> and <m>A\adjoint{A}</m> are Hermitian matrices (<acroref type="definition" acro="HM" />), and hence are normal (Definition<nbsp /><xref ref="definition-normal-matrix" />).  This provides the existence of orthonormal bases of eigenvectors for each matrix by <acroref type="theorem" acro="OBNM" />.  Also, since each matrix is diagonalizable (<acroref type="definition" acro="DZM" />) by <acroref type="theorem" acro="OD" /> the algebraic and geometric multiplicities of each eigenvalue (zero or not) are equal by <acroref type="theorem" acro="DMFE" />.</p>

            <p>Our first step is to establish that a nonzero eigenvalue <m>\lambda</m> has the same geometric multiplicity for both <m>\adjoint{A}A</m> and <m>A\adjoint{A}</m>.   Suppose <m>\set{\vectorlist{x}{s}}</m> is an orthonormal basis of eigenvectors of <m>\adjoint{A}A</m> for the eigenspace <m>\eigenspace{\adjoint{A}A}{\lambda}</m>.  Then for <m>1\leq i\lt j\leq s</m>,
            <me>\innerproduct{A\vect{x}_i}{A\vect{x}_j}=\innerproduct{\vect{x}_i}{\adjoint{A}A\vect{x}_j}=\innerproduct{\vect{x}_i}{\lambda\vect{x}_j}=\lambda\innerproduct{\vect{x}_i}{\vect{x}_j}=\lambda(0)</me></p>

            <p>So the set <m>E=\set{A\vect{x}_1,\,A\vect{x}_2,\,A\vect{x}_3,\,\dots,\,A\vect{x}_s}</m> is an orthogonal set of nonzero eigenvectors of <m>A\adjoint{A}</m> for the eigenvalue <m>\lambda</m>.  By <acroref type="theorem" acro="OSLI" />, the set <m>E</m> is linearly independent and so the geometric multiplicity of <m>\lambda</m> as an eigenvalue of <m>A\adjoint{A}</m> is <m>s</m> or greater.  We have
            <me>\algmult{\adjoint{A}A}{\lambda} = \geomult{\adjoint{A}A}{\lambda} \leq \geomult{A\adjoint{A}}{\lambda} = \algmult{A\adjoint{A}}{\lambda}</me>
            This inequality applies to any matrix for any of its nonzero eigenvalues.  We now apply it to the matrix <m>\adjoint{A}</m>,
            <me>\algmult{A\adjoint{A}}{\lambda} = \algmult{\adjoint{\left(\adjoint{A}\right)}\adjoint{A}}{\lambda} \leq \algmult{\adjoint{A}\adjoint{\left(\adjoint{A}\right)}}{\lambda} = \algmult{\adjoint{A}A}{\lambda}</me>
            With the twin inequalities, we see that the four multiplicities of a common nonzero eigenvalue of <m>\adjoint{A}A</m> and <m>A\adjoint{A}</m> are all equal.  This is enough to establish that <m>p=q</m>, since we cannot have an eigenvalue of one of the matrix-adjoint products along with a zero algebraic multiplicity for the other matrix-adjoint product. Then the eigenvalues can be ordered such that <m>\lambda_i=\rho_i</m> for <m>1\leq i\leq p</m>.</p>

            <p>For any matrix, the null space is identical to the eigenspace of the zero eigenvalue, and thus the nullity of the matrix is equal to the geometric multiplicity of the zero eigenvalue.  As our matrix-adjoint products are diagonalizable, the nullity is equal to the algebraic multiplicity of the zero eigenvalue.  The algebraic multiplicities of all the eigenvalues sum to the size of the matrix (<acroref type="theorem" acro="NEM" />), as does the rank and nullity (<acroref type="theorem" acro="RPNC" />).  So the rank of our matrix-adjoint products is equal to the sum of the algebraic multiplicities of the nonzero eigenvalues.  So the ranks of <m>\adjoint{A}A</m> and <m>A\adjoint{A}</m> are equal, <me>\rank{\adjoint{A}A}=\sum_{i=1}^{p}\algmult{\adjoint{A}A}{\lambda_i}=\sum_{i=1}^{q}\algmult{A\adjoint{A}}{\rho_i}=\rank{A\adjoint{A}}</me></p>

            <p>When <m>A</m> is rectangular, the square matrices <m>\adjoint{A}A</m> and <m>A\adjoint{A}</m> have different sizes.  With equal algebraic and geometric multiplicities for their common nonzero eigenvalues, the difference in their sizes is manifest in different algebraic multiplicities for the zero eigenvalue and different nullities.  Specifically,
            <md>
                <mrow>\nullity{\adjoint{A}A}&amp;=n-r &amp; \nullity{A\adjoint{A}}&amp;=m-r</mrow>
            </md></p>

            <p>Suppose that <m>\vectorlist{x}{n}</m> is an orthonormal basis of <m>\complex{n}</m> composed of eigenvectors of <m>\adjoint{A}A</m> and ordered so that <m>\vect{x}_i</m>, <m>r+1\leq i\leq n</m> are eigenvectors of <m>\adjoint{A}A</m> for the zero eigenvalue.  Denote the associated nonzero eigenvalues of <m>\adjoint{A}A</m> for these eigenvectors by <m>\vect{\delta}_i</m>, <m>1\leq i\leq r</m>.   Then define <me>\vect{y}_i=\frac{1}{\sqrt{\delta_i}}A\vect{x}_i,\quad 1\leq i\leq r</me>
            Let <m>\vect{y}_{r+1},\,\vect{y}_{r+2},\,\vect{y}_{r+2},\,\dots,\,\vect{y}_{m}</m> be an orthonormal basis for the eigenspace <m>\eigenspace{A\adjoint{A}}{0}</m>, whose existence is guaranteed by the Gram-Schmidt process (<acroref type="theorem" acro="GSP" />).  As scalar multiples of demonstrated eigenvectors of <m>A\adjoint{A}</m>, <m>\vect{y}_i</m>, <m>1\leq i\leq r</m> are also eigenvectors of <m>A\adjoint{A}</m>, and <m>\vect{y}_i</m>, <m>r+1\leq i\leq n</m> have been chosen as eigenvectors of <m>A\adjoint{A}</m>.  These eigenvectors also have norm 1, as we now show.</p>

            <p>For <m>1\leq i\leq r</m>,
            <md>
                <mrow>\norm{\vect{y}_i}^2&amp;=\norm{\frac{1}{\sqrt{\delta_i}}A\vect{x}_i}^2=\innerproduct{\frac{1}{\sqrt{\delta_i}}A\vect{x}_i}{\frac{1}{\sqrt{\delta_i}}A\vect{x}_i}</mrow> 
                <mrow>&amp;=\conjugate{\frac{1}{\sqrt{\delta_i}}}\frac{1}{\sqrt{\delta_i}}\innerproduct{A\vect{x}_i}{A\vect{x}_i}=\frac{1}{\delta_i}\innerproduct{A\vect{x}_i}{A\vect{x}_i}</mrow> 
                <mrow>&amp;=\frac{1}{\delta_i}\innerproduct{\vect{x}_i}{\adjoint{A}A\vect{x}_i}=\frac{1}{\delta_i}\innerproduct{\vect{x}_i}{\delta_i\vect{x}_i}</mrow>
                <mrow>&amp;=\frac{1}{\delta_i}\delta_i\innerproduct{\vect{x}_i}{\vect{x}_i}=1 </mrow>
            </md></p>

            <p>For <m>r+1\leq i\leq n</m>, the <m>\vect{y}_i</m> have been chosen to have norm 1.</p>

            <p>Finally we check orthogonality.  Consider two eigenvectors <m>\vect{y}_i</m> and <m>\vect{y}_j</m> with <m>1\leq i\lt j\leq m</m>.  If these two vectors have different eigenvalues, then <acroref type="theorem" acro="HMOE" /> establishes that the two eigenvectors are orthogonal.  If the two eigenvectors have a zero eigenvalue, then they are orthogonal by the choice of the orthonormal basis of <m>\eigenspace{A\adjoint{A}}{0}</m>.
            If the two eigenvectors have identical eigenvalues, which are nonzero, then
            <md>
                <mrow>\innerproduct{\vect{y}_i}{\vect{y}_j}&amp;= \innerproduct{\frac{1}{\sqrt{\delta_i}}A\vect{x}_i}{\frac{1}{\sqrt{\delta_j}}A\vect{x}_j}=\conjugate{\frac{1}{\sqrt{\delta_i}}}\frac{1}{\sqrt{\delta_j}} \innerproduct{A\vect{x}_i}{A\vect{x}_j}</mrow>
                <mrow>&amp;=\frac{1}{\sqrt{\delta_i\delta_j}}\innerproduct{A\vect{x}_i}{A\vect{x}_j}=\frac{1}{\sqrt{\delta_i\delta_j}} \innerproduct{\vect{x}_i}{\adjoint{A}A\vect{x}_j}</mrow>
                <mrow>&amp;=\frac{1}{\sqrt{\delta_i\delta_j}} \innerproduct{\vect{x}_i}{\delta_j\vect{x}_j}= \frac{\delta_j}{\sqrt{\delta_i\delta_j}} \innerproduct{\vect{x}_i}{\vect{x}_j}</mrow>
                <mrow>&amp;=\frac{\delta_j}{\sqrt{\delta_i\delta_j}}(0)=0</mrow>
            </md></p>

            <p>So <m>\set{\vectorlist{y}{m}}</m> is an orthonormal set of eigenvectors for <m>A\adjoint{A}</m>.  The critical relationship between these two orthonormal bases is present by design.  For <m>1\leq i\leq r</m>, <me> A\vect{x}_i =\sqrt{\delta_i}\frac{1}{\sqrt{\delta_i}}A\vect{x}_i =\sqrt{\delta_i}\vect{y}_i</me>
            For <m>r+1\leq i\leq n</m> we have <me>\innerproduct{A\vect{x}_i}{A\vect{x}_i}=\innerproduct{\vect{x}_i}{\adjoint{A}A\vect{x}_i}=\innerproduct{\vect{x}_i}{\zerovector}=0</me>
            So by <acroref type="theorem" acro="PIP" />, <m>A\vect{x}_i=\zerovector</m>.</p>
            </proof>
        </theorem>
    </subsection>

    <subsection acro="SVD">
        <title>Singular Value Decomposition</title>

        <p>The square roots of the eigenvalues of <m>\adjoint{A}A</m> (or almost equivalently, <m>A\adjoint{A}</m>!) are known as the singular values of <m>A</m>.  Here is the definition.</p>

        <definition acro="SV" index="singular values">
            <title>Singular Values</title>
            <statement>
                <p>Suppose <m>A</m> is an <m>m\times n</m> matrix.  If the eigenvalues of <m>\adjoint{A}A</m> are <m>\scalarlist{\delta}{n}</m>, then the <term>singular values</term> of <m>A</m> are <me>\sqrt{\delta_1},\,\sqrt{\delta_2},\,\sqrt{\delta_3},\,\ldots,\,\sqrt{\delta_n}</me></p>
            </statement>
        </definition>

        <p>Theorem<nbsp /><xref ref="eigenvalues-matrix-adjoint-product" /> is a total setup for the singular value decomposition.  This remarkable theorem says that <em>any</em> matrix can be broken into a product of three matrices.  Two are square and unitary.  In light of <acroref type="theorem" acro="UMPIP" />, we can view these matrices as transforming vectors or coordinates in a rotational fashion.  The middle matrix of this decomposition is rectangular, but is as close to being diagonal as a rectangular matrix can be.  Viewed as a transformation, this matrix effects, reflections, contractions, or expansions along axes <mdash /> it stretches vectors.  So any matrix, viewed as a geometric transformation is the product of a rotation, a stretch and a rotation.</p>

        <p>The singular value theorem can also be viewed as an application of our most general statement about matrix representations of linear transformations relative to different bases.  <acroref type="theorem" acro="MRCB" /> concerns linear transformations <m>\ltdefn{T}{U}{V}</m> where <m>U</m> and <m>V</m> are possibly different vector spaces.  When <m>U</m> and <m>V</m> have different dimensions, the resulting matrix representation will be rectangular.  In <acroref type="section" acro="CB" /> we quickly specialized to the case where <m>U=V</m>, and the matrix representations are square, with one of our most central results, <acroref type="theorem" acro="SCB" />.  Theorem<nbsp /><xref ref="theorem-singular-value-decomposition" />, next, is an application of the full generality of <acroref type="theorem" acro="MRCB" /> where the relevant bases are now orthonormal sets.</p>

        <theorem xml:id="theorem-singular-value-decomposition">   <!-- was SVD -->
            <title>Singular Value Decomposition</title>
            <statement>
                <p>Suppose <m>A</m> is an <m>m\times n</m> matrix of rank <m>r</m> with nonzero singular values <m>\scalarlist{s}{r}</m>.  Then <m>A=US\adjoint{V}</m> where <m>U</m> is a unitary matrix of size <m>m</m>, <m>V</m> is a unitary matrix of size <m>n</m> and <m>S</m> is an <m>m\times n</m> matrix given by <me>\matrixentry{S}{ij}=
                \begin{cases}
                s_i&amp;\text{if }1\leq i=j\leq r\\
                0&amp;\text{otherwise} \end{cases}</me></p>
            </statement>

            <proof>
                <p>Let <m>\vectorlist{x}{n}</m> and <m>\vectorlist{y}{m}</m> be the orthonormal bases described by the conclusion of Theorem<nbsp /><xref ref="eigenvalues-matrix-adjoint-product" />.  Define <m>U</m> to be the <m>m\times m</m> matrix whose columns are <m>\vect{y}_i</m>, <m>1\leq i\leq m</m>, and define <m>V</m> to be the <m>n\times n</m> matrix whose columns are <m>\vect{x}_i</m>, <m>1\leq i\leq n</m>.  With orthonormal sets of columns, both <m>U</m> and <m>V</m> are unitary matrices by <acroref type="theorem" acro="CUMOS" />.</p>

                <p>Then for <m>1\leq i\leq m</m>, <m>1\leq j\leq n</m>,
                <md>
                    <mrow>\matrixentry{AV}{ij}&amp;=\vectorentry{A\vect{x}_j}{i}=\vectorentry{\sqrt{\delta_j}\vect{y}_j}{i}=s_j\vectorentry{\vect{y}_j}{i}</mrow>
                    <mrow>&amp;=\matrixentry{U}{ij}\matrixentry{S}{jj}=\sum_{k=1}^{m}\matrixentry{U}{ik}\matrixentry{S}{kj}=\matrixentry{US}{ij}</mrow>
                </md></p>

                <!-- TODO: SVD proof may need more care with zero columns? -->

                <p>So by <acroref type="theorem" acro="ME" />, <m>AV</m> and <m>US</m> are equal matrices, <m>AV=US</m>.  <m>V</m> is unitary, so applying its inverse yields the decomposition <me>A=US\adjoint{V}</me></p>
            </proof>
        </theorem>

        <p>Typically, the singular values of a matrix are ordered from largest to smallest, so this convention is used for the diagonal elements of the matrix <m>S</m> in the decomposition, and then the columns of <m>U</m> and <m>V</m> will be ordered similarly.</p>
    </subsection>

    <subsection xml:id="subsection-visualizing-SVD">
        <title>Visualizing the SVD</title>

        <p>It is helpful to think of the orthonormal bases that are the columns of <m>U</m> and <m>V</m> as <q>coordinate systems,</q> since they are pairwise <q>perpendicular</q> unit vectors, like the <m>\vec{i},\,\vec{j},\,\vec{k}</m> often used in describing the geometry of space.  We would then call each basis vector an <q>axis</q>.</p>

        <p>Now think of an <m>m\times n</m> matrix as a function from <m>\complex{n}</m> to <m>\complex{m}</m>.  For an input vector <m>\vect{w}\in\complex{n}</m>, we have the output vector <m>\vect{y}=A\vect{w}\in\complex{m}</m>.  If we write the output vector using the SVD decomposition of <m>A</m> as <m>A\vect{w}=US\adjoint{V}\vect{w}</m> we can consider the output as a three-step process that is more formally a composition of three linear transformations.  Recall that unitary matrices preserve inner products, and thus preserve norms (length) and relative positions of two vectors (the <q>angle</q> between vectors), which is why unitary matrices are sometimes called <q>isometries</q> (<acroref type="theorem" acro="UMPIP" />). <ol>
            <li><m>\adjoint{V}</m> is the inverse of <m>V</m> so it will take any basis vector <m>\vect{x}_i</m> to a column of the identity matrix <m>\vect{e}_i</m>, an element of the standard basis of <m>\complex{n}</m>, or and axis of the <q>usual</q> coordinate system.  Any other vector, such as our <m>\vect{w}</m>, will have its length preserved, but changing its position, though its position relative to the axes is unchanged.</li>

            <li><m>S</m> converts the <q>rotated</q> <m>\vect{w}</m> from <m>\complex{n}</m> to <m>\complex{m}</m>.  But it does so in a very simple way.  It simply scales each entry of the vector by a positive amount using a different singular value for each entry.  If <m>m\gt n</m>, then the extra entries are simply new zeros.  If <m>m\lt n</m>, then some entries get discarded.</li>

            <li><m>U</m> will convert the standard basis vectors (the usual axes) to the new orthonormal basis given by the <m>\vect{y}_i</m>.  The twice-transformed version of <m>\vect{w}</m> will have its length preserved, but change position, though its position relative to the axes is unchanged.</li>
        </ol>  So, <em>every</em> matrix is a rotation, a stretch, and a rotation.  That is a simple, but accurate, understanding of what the SVD tells us.</p>

        <p>Here is another look at the same idea.  Consider the columns of <m>U</m> and <m>V</m> again as the axes of new coordinate systems.  Then their adjoints are their inverses and each take the new axes to the standard unit vectors (columns of the identity matrix), the axes of the usual coordinate system.  Consider an input vector <m>\vect{w}</m>, and its output <m>\vect{y}=A\vect{w}</m>, relative to these new bases and convert each to the standard coordinate system, <m>\vect{w}^\prime=\adjoint{V}\vect{w}</m> and <m>\vect{y}^\prime=\adjoint{U}\vect{y}</m>.  Then <me>\vect{y}^\prime=\adjoint{U}\vect{y}=\adjoint{U}A\vect{w}=\adjoint{U}US\adjoint{V}\vect{w}=S\vect{w}^\prime</me>
        In the <q>primed</q> spaces, the action of <m>A</m> is very simple.  Or in other words, every linear transformation has a matrix representation that is basically diagonal, if only we pick the right bases for the domain and codomain.</p>

    </subsection>

    <subsection xml:id="subsection-properties-of-SVD">
        <title>Properties of the SVD</title>

        <p>The appeal of the singular value decomposition is two-fold.  First it is applicable to <em>any</em> matrix.  That should be obvious enough.  Second, components of the SVD provide a wealth of information about a matrix, and in the case of numerical matrices, they are well-behaved.  In this subsection we collect various theorems about the SVD, and explore the consequences in a section about applications, Section<nbsp /><xref provisional="section-applications-of-SVD" />.</p>

        <p>The SVD gives a decomposition of a matrix of a sum of rank one matrices, and the magnitude of the singular values tells us which of these rank one matrices is the most <q>important</q>.</p>

        <theorem xml:id="theorem-svd-rank-one-decomposition">
            <title>SVD Rank One Decomposition</title>

            <statement>
                <p>Suppose the singular value decomposition of an <m>m\times n</m> matrix <m>A</m> is given by <m>A=US\adjoint{V}</m>, where the nonzero singular values in the first <m>r</m> entries of the diagonal of <m>S</m> are <m>\scalarlist{s}{r}</m>, and the columns of <m>U</m> and <m>V</m> are, respectively, <m>\vectorlist{y}{m}</m> and <m>\vectorlist{x}{n}</m>.  Then <me>A=\sum_{i=1}^r\,s_i\,\vect{y}_i\adjoint{\vect{x}_i}</me></p>
            </statement>

            <proof>
                <p>As usual, let <m>\vect{e}_i</m> be column <m>i</m> of the identity matrix <m>I_m</m>.  Define <m>S_i</m>, for <m>1\leq i\leq r</m> to be the <m>m\times n</m> matrix where every entry is zero, except column <m>i</m> is <m>s_i\vect{e}_i</m>.  Then, by design, <m>S=\sum_{i=1}^r\,S_i</m>.  We have, <md>
                    <mrow>
                        A
                        &amp;=US\adjoint{V}=U\,\sum_{i=1}^r\,S_i\,\adjoint{V}
                             =\sum_{i=1}^rUS_i\adjoint{V}
                    </mrow>
                    <mrow>
                        &amp;=\sum_{i=1}^r\,U\left\lbrack\zerovector|\dots|s_i\vect{e}_i|\dots|\zerovector\right\rbrack\adjoint{V}
                        =\sum_{i=1}^r\,s_i\left\lbrack\zerovector|\dots|U\vect{e}_i|\dots|\zerovector\right\rbrack\adjoint{V}
                    </mrow>
                    <mrow>
                        &amp;=\sum_{i=1}^r\,s_i\left\lbrack\zerovector|\dots|\vect{y}_i|\dots|\zerovector\right\rbrack\adjoint{V}
                             =\sum_{i=1}^r\,s_i\vect{y}_i\adjoint{\vect{x}_i}
                    </mrow>
                </md>
                </p>
            </proof>
        </theorem>

        <p>Be sure to recognize <m>\vect{y}_i\adjoint{\vect{x}_i}</m> as the outer product, an <m>m\times n</m> matrix of rank one (every row is a multiple of every other row, and similarly for columns).  See Subsection<nbsp /><xref provisional="subsection-SVD-image-compression" /> for a good example of the utility of this result.</p>

        <!-- TODO: Properties of SVD needs (numerical) rank discussion -->
        <!-- TODO: Properties of SVD needs range and kernel theorem (as spans of x, y vectors) -->

    </subsection>

</section>
